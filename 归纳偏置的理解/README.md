>自来熟这里就是你熟起来太快，很有索取的嫌疑，所以这里很容易将你踢开。
# 1.ML中, 很多学习算法经常会对学习的问题做一些假设, 这些假设本质就是称之为inductive bias

首先理解归纳induction，这个是自然科学中常用的两大方法之一(归纳与演绎，induction and deduction), 指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；偏置(Bias)是指我们对模型的偏好。

# 2.归纳偏置可以理解为
>我们从现实生活中观察到的现象中归纳出一定的规则heuristics, 然后对模型做一定的约束，从而可以起到**模型选择的作用。**

**从假设空间中选择出更加符合现实规则的模型，其实贝叶斯学习中的先验prior这个叫法,可能比归纳偏置更加客观一些。**

# 3.inductive bias在机器学习中几乎无处不可见。
>奥卡姆剃刀原理,即希望学习到的模型复杂度lower, 这个就是一种inductive bias。
>或者一些更强的假设: KNN中假设特征空间中相邻的样本倾向于属于同一类;

>SVM中假设好的分类器应该最大化类别边界距离,等等。

## 深度学习方面也是一样
>NN为例, 所有的网络结构/组件/机制往往就来源于归纳偏置。

*在卷积神经网络中,我们假设具有局部性locality 的特性,就是当我们把相邻的一些feat放在一些，这里就是更加容易获得解。*

这些什么注意力机制就是我们从人的直觉/生活经验归纳得到的规则。

<img width="679" alt="image" src="https://user-images.githubusercontent.com/40928887/124966842-54d53400-e056-11eb-976c-b588262046ce.png">

**为了让我们选择更好的模型**

**我们的很多工作的创新不都是偏向于一个新的inductive bias吗**

*将无限可能的目标函数约束在一个有限的类别之中，这样模型的学习才成为可能。*

>如果我们放轻松我们的模型假设，就是比较弱的inductive bias这样的模型可能更可能强力，但是就是容易过拟合。

