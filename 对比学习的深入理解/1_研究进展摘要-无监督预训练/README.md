对比学习(Contrastive Learning)最近一年比较火，各路大神比如Hinton、Yann LeCun、Kaiming He及一流研究机构比如Facebook、Google、DeepMind，都投入其中并快速提出各种改进模型：Moco系列、SimCLR系列、BYOL、SwAV…..，各种方法相互借鉴，又各有创新，俨然一场机器学习领域的军备竞赛。对比学习属于无监督或者自监督学习，但是目前多个模型的效果已超过了有监督模型，这样的结果很令人振奋。

>对比学习属于无监督或者自监督学习，但是目前多个模型的效果已经超过了**有监督学习，这样的结果很令人振奋。**

# 1.BERT成功的推进
NLP的Bert模型对于**这波图像领域的对比学习热潮**，是具有启发和推动作用的。我们知道，Bert预训练模型，通过MLM任务的自监督学习，充分挖掘了模型从海量无标注文本中学习通用知识的能力。而图像领域的预训练，往往是有监督的，就是用ImageNet来进行预训练，但是在下游任务中Fine-tuning的效果，跟Bert在NLP下游任务中带来的性能提升，是没法比的。

>NLP做自监督成功了，图像领域难道就不可以成功吗？

**这个就是让图像领域从有监督预训练到自监督预训练。**

<img width="665" alt="image" src="https://user-images.githubusercontent.com/40928887/127965507-d189cef0-8f02-410a-8b68-ade7d4fd69c5.png">

>图像想要发展也得沿用这个策略。

# 2.对比学习就是想要干NLP类似BERT的事情
对比学习是自监督学习的一种，也就是说，不依赖标注数据，要从无标注图像中自己学习知识。

>自监督学习其实在图像中已经被探索很久了，两大类比如生成式自监督学习和判别式自监督学习。

1)generative: VAE/GAN, 即它要求模型重建图像或者图像的一部分，这类型的任务难度相对比较高，要求像素级的重构，中间的图像编码必须包含很多细节信息。

2)discriminative: 对比学习，这个任务难度要低一些，**无明确定义、有指导原则**。

>它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远

1.如何构造相似实例;

2.如何构造不相似实例;

3.如何构造能够遵循上述指导原则的表示学习模型结构

4.如何防止模型坍塌(Model Collapse)


<img width="668" alt="image" src="https://user-images.githubusercontent.com/40928887/127965776-9b63deeb-3967-413e-a3c1-d231e99399f9.png">

