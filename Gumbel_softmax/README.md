# 1.强化学习
这个就是如果动作空间是离散的, 比如*上/下/左/右*四个动作，通常的做法就是网络传输出一个4d的one-hot vector, 分别代表四个动作.

>比如[1,0,0,0]代表上, [0,1,0,0]代表下等等。

那么咱们到底应该取哪个动作呢？

>如果是deterministic的话，这个就是根据输出的每个dim的大小，选择这个值最大的作为输出动作,即arg⁡max(v)。

>例如网络输出的四维向量为v=[−20,10,9.6,6.2]，第二个维度取到最大值10，那么输出的动作就是[0,1,0,0]，这和多类别的分类任务是一个道理

**但是对于分类任务来说，argmax仅仅只用在inference求具体类别的过程中，不需要计算梯度; 但是对于DRL来说，argmax之后还需要计算梯度。**

>这个直接argmax在网络train的过程中有两个很严重的问题:

1.argmax不能计算梯度，也就不能更新网络

2.而且输出并不代表概率意义，只是单纯的argmax没有探索性。

我们为了获得argmax和argmin的导数，使得这一步是可以训练的，我们就可以使用这样的一个重参数的技巧。@gumbel-max trick

# 2.Gumbel-Softmax Trick
1.正向传播
对于n维概率向量π,对π对应的离散随机变量π添加Gumbel噪声，再采样：
